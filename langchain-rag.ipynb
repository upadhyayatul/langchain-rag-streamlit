import streamlit as st
import PyPDF2
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_mistralai import ChatMistralAI
from langchain_mistralai import MistralAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain_cohere import ChatCohere
from langchain_cohere import CohereEmbeddings
from langchain.chains.retrieval import create_retrieval_chain
import os
from dotenv import load_dotenv

load_dotenv()
os.environ['MISTRAL_API_KEY'] = os.getenv('MISTRAL_API_KEY')
os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')
os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY')

pdf_file = st.file_uploader("Choose a file", accept_multiple_files=False, type='pdf')
input_text = st.text_input('Ask your question:')

if pdf_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(pdf_file.read())
        tmp_path =  tmp_file.name

    loader = PyPDFLoader(tmp_path)
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    final_docs = text_splitter.split_documents(docs)
    
    #embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    #embeddings = MistralAIEmbeddings(model='mistral-embed')
    embeddings = CohereEmbeddings(model="embed-v4.0")
    vector_db = FAISS.from_documents(final_docs, embeddings)
    
    prompt = ChatPromptTemplate.from_template(
        """
        Answer the following question only from the provided context:
        <context> {context} </context>
        """
    )

    #llm = ChatOpenAI(model='gpt-4.1-nano')
    #llm = ChatMistralAI(model='mistral-large-latest')
    llm = ChatCohere(model='command-a-03-2025')
    doc_chain = create_stuff_documents_chain(llm, prompt)

    retriever = vector_db.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, doc_chain)
    if input_text:
        response = retrieval_chain.invoke({'input':input_text})
        st.subheader('Response:')
        st.write(response['answer'])
